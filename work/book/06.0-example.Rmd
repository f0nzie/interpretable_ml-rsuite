# Example-Based Explanations {#example-based}

Example-based explanation methods select particular instances of the dataset to explain the behavior of machine learning models or to explain the underlying data distribution.

<!-- *Keywords: example-based explanations, case-based reasoning (CBR), solving by analogy* -->

Example-based explanations are mostly model-agnostic, because they make any machine learning model more interpretable.
The difference to model-agnostic methods is that the example-based methods explain a model by selecting instances of the dataset and not by creating summaries of features (such as [feature importance](#feature-importance) or [partial dependence](#pdp)).
Example-based explanations only make sense if we can represent an instance of the data in a humanly understandable way.
This works well for images, because we can view them directly.
In general, example-based methods work well if the feature values of an instance carry more context, meaning the data has a structure, like images or texts do.
It is more challenging to represent tabular data in a meaningful way, because an instance can consist of hundreds or thousands of (less structured) features.
Listing all feature values to describe an instance is usually not useful.
It works well if there are only a handful of features or if we have a way to summarize an instance.


Example-based explanations help humans construct mental models of the machine learning model and the data the machine learning model has been trained on.
It especially helps to understand complex data distributions.
But what do I mean by example-based explanations?
We often use them in our jobs and daily lives.
Let us start with some examples[^cbr].

A physician sees a patient with an unusual cough and a mild fever.
The patient's symptoms remind her of another patient she had years ago with similar symptoms. 
She suspects that her current patient could have the same disease and she takes a blood sample to test for this specific disease.

A data scientist works on a new project for one of his clients:
Analysis of the risk factors that lead to the failure of production machines for keyboards.
The data scientist remembers a similar project he worked on and reuses parts of the code from the old project because he thinks the client wants the same analysis.

A kitten sits on the window ledge of a burning and uninhabited house. 
The fire department has already arrived and one of the firefighters ponders for a second whether he can risk going into the building to save the kitten.
He remembers similar cases in his life as a firefighter: 
Old wooden houses that have been burning slowly for some time were often unstable and eventually collapsed.
Because of the similarity of this case, he decides not to enter, because the risk of the house collapsing is too great. 
Fortunately, the kitty jumps out of the window, lands safely and nobody is harmed in the fire. Happy end.

These stories illustrate how we humans think in examples or analogies.
The blueprint of example-based explanations is: 
Thing B is similar to thing A and A caused Y, so I predict that B will cause Y as well.
Implicitly, some machine learning approaches work example-based.
[Decision trees](#tree) partition the data into nodes based on the similarities of the data points in the features that are important for predicting the target.
A decision tree gets the prediction for a new data instance by finding the instances that are similar (= in the same terminal node) and returning the average of the outcomes of those instances as the prediction.
The k-nearest neighbors (knn) method works explicitly with example-based predictions. 
For a new instance, a knn model locates the k-nearest neighbors (e.g. the k=3 closest instances) and returns the average of the outcomes of those neighbors as a prediction.
The prediction of a knn can be explained by returning the k neighbors, which -- again -- is only meaningful if we have a good way to represent a single instance.

The chapters in this part cover the following example-based interpretation methods:

- [Counterfactual explanations](#counterfactual) tell us how an instance has to change to significantly change its prediction. 
By creating counterfactual instances, we learn  about how the model makes its predictions and can explain individual predictions.
- [Adversarial examples](#adversarial) are counterfactuals used to fool machine learning models. 
The emphasis is on flipping the prediction and not explaining it. 
- [Prototypes](#proto) are a selection of representative instances from the data and criticisms are instances that are not well represented by those prototypes. [^critique]
- [Influential instances](#influential) are the training data points that were the most influential for the parameters of a prediction model or the predictions themselves. 
Identifying and analysing influential instances helps to find problems with the data, debug the model and understand the model's behavior better.
- [k-nearest neighbors model](#other-interpretable): An (interpretable) machine learning model  based on examples.


[^cbr]: Aamodt, Agnar, and Enric Plaza. "Case-based reasoning: Foundational issues, methodological variations, and system approaches." AI communications 7.1 (1994): 39-59.

